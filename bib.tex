\bibitem{Loshchilov} Loshchilov, Ilya, and Frank Hutter. ‘Decoupled Weight Decay Regularization’. 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. https://arxiv.org/pdf/1711.05101.pdf

%Not used but was helpful
\bibitem{Agarwal} \footnote[1]{\label{note1}These sources were not directly referenced in the paper, but were crucial in understand the concepts better (Adaptive Methods, Adam, etc.) in order to create the problem set.} \hspace{-6mm} Agarwal, Naman \& Bullins, Brian \& Chen, Xinyi \& Hazan, Elad \& Singh, Karan \& Zhang, Cyril \& Zhang, Yi. (2018). The Case for Full-Matrix Adaptive Regularization. https://arxiv.org/pdf/1806.02958.pdf

%Not used but was helpful
\bibitem{Bushaev} \footnotemark[\ref{note1}]Bushaev, Vitaly. (2018). Adam — latest trends in deep learning optimization. https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c

%Not used but was helpful
\bibitem{Jiang} \footnotemark[\ref{note1}]Jiang, Lili. (2020). A Visual Explanation of Gradient Descent Methods (Momentum, AdaGrad, RMSProp, Adam). https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c

%Not used but was helpful
\bibitem{Villarraga} \footnotemark[\ref{note1}]Villarraga, Daniel. (2021). AdaGrad. https://optimization.cbe.cornell.edu/index.php?title=AdaGrad

\bibitem{Zhang} \footnotemark[\ref{note1}]Zhang, Jiawei. (2019). Gradient Descent based Optimization Algorithms for Deep Learning Models Training. https://arxiv.org/pdf/1903.03614.pdf

\bibitem{ZhangBERT} \footnotemark[\ref{note1}]Zhang, T., Wu, F., Katiyar, A., Weinberger, K., & Artzi, Y.. (2020). Revisiting Few-sample BERT Fine-tuning. https://arxiv.org/pdf/2006.05987.pdf